{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kpi.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"./models/ramses_kpi/tmp/checkpoint\"\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664 documents\n",
      "29 classes\n",
      "66 unique stemmed words\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               8576      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 29)                1885      \n",
      "=================================================================\n",
      "Total params: 18,717\n",
      "Trainable params: 18,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " None \n",
      "\n",
      "Train on 664 samples\n",
      "Epoch 1/200\n",
      "660/664 [============================>.] - ETA: 0s - loss: 3.0069 - accuracy: 0.1348\n",
      "Epoch 00001: loss improved from inf to 3.00943, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 2s 3ms/sample - loss: 3.0094 - accuracy: 0.1340\n",
      "Epoch 2/200\n",
      "630/664 [===========================>..] - ETA: 0s - loss: 2.3447 - accuracy: 0.3032\n",
      "Epoch 00002: loss improved from 3.00943 to 2.32029, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 2.3203 - accuracy: 0.3087\n",
      "Epoch 3/200\n",
      "645/664 [============================>.] - ETA: 0s - loss: 1.8062 - accuracy: 0.4078\n",
      "Epoch 00003: loss improved from 2.32029 to 1.79709, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 1ms/sample - loss: 1.7971 - accuracy: 0.4021\n",
      "Epoch 4/200\n",
      "645/664 [============================>.] - ETA: 0s - loss: 1.3558 - accuracy: 0.5938\n",
      "Epoch 00004: loss improved from 1.79709 to 1.35654, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 1.3565 - accuracy: 0.5949\n",
      "Epoch 5/200\n",
      "640/664 [===========================>..] - ETA: 0s - loss: 1.0388 - accuracy: 0.6891\n",
      "Epoch 00005: loss improved from 1.35654 to 1.04050, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 1.0405 - accuracy: 0.6867\n",
      "Epoch 6/200\n",
      "640/664 [===========================>..] - ETA: 0s - loss: 0.8480 - accuracy: 0.7406\n",
      "Epoch 00006: loss improved from 1.04050 to 0.83789, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 1ms/sample - loss: 0.8379 - accuracy: 0.7440\n",
      "Epoch 7/200\n",
      "650/664 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.7785\n",
      "Epoch 00007: loss improved from 0.83789 to 0.68067, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.6807 - accuracy: 0.7771\n",
      "Epoch 8/200\n",
      "645/664 [============================>.] - ETA: 0s - loss: 0.5957 - accuracy: 0.8403\n",
      "Epoch 00008: loss improved from 0.68067 to 0.58955, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.5896 - accuracy: 0.8404\n",
      "Epoch 9/200\n",
      "645/664 [============================>.] - ETA: 0s - loss: 0.5199 - accuracy: 0.8310\n",
      "Epoch 00009: loss improved from 0.58955 to 0.51319, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.5132 - accuracy: 0.8343\n",
      "Epoch 10/200\n",
      "660/664 [============================>.] - ETA: 0s - loss: 0.4681 - accuracy: 0.8424\n",
      "Epoch 00010: loss improved from 0.51319 to 0.46832, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.4683 - accuracy: 0.8419\n",
      "Epoch 11/200\n",
      "650/664 [============================>.] - ETA: 0s - loss: 0.4429 - accuracy: 0.8569\n",
      "Epoch 00011: loss improved from 0.46832 to 0.44458, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.4446 - accuracy: 0.8569\n",
      "Epoch 12/200\n",
      "650/664 [============================>.] - ETA: 0s - loss: 0.4018 - accuracy: 0.8692\n",
      "Epoch 00012: loss improved from 0.44458 to 0.40738, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.4074 - accuracy: 0.8675\n",
      "Epoch 13/200\n",
      "635/664 [===========================>..] - ETA: 0s - loss: 0.3494 - accuracy: 0.8835\n",
      "Epoch 00013: loss improved from 0.40738 to 0.35744, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.3574 - accuracy: 0.8840\n",
      "Epoch 14/200\n",
      "630/664 [===========================>..] - ETA: 0s - loss: 0.3728 - accuracy: 0.8746\n",
      "Epoch 00014: loss did not improve from 0.35744\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.3640 - accuracy: 0.8780\n",
      "Epoch 15/200\n",
      "660/664 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9197\n",
      "Epoch 00015: loss improved from 0.35744 to 0.26957, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2696 - accuracy: 0.9172\n",
      "Epoch 16/200\n",
      "630/664 [===========================>..] - ETA: 0s - loss: 0.2735 - accuracy: 0.9016\n",
      "Epoch 00016: loss did not improve from 0.26957\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2760 - accuracy: 0.9006\n",
      "Epoch 17/200\n",
      "640/664 [===========================>..] - ETA: 0s - loss: 0.3075 - accuracy: 0.8953\n",
      "Epoch 00017: loss did not improve from 0.26957\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.3103 - accuracy: 0.8931\n",
      "Epoch 18/200\n",
      "650/664 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.9077\n",
      "Epoch 00018: loss did not improve from 0.26957\n",
      "664/664 [==============================] - 1s 1ms/sample - loss: 0.2822 - accuracy: 0.9096\n",
      "Epoch 19/200\n",
      "640/664 [===========================>..] - ETA: 0s - loss: 0.3102 - accuracy: 0.8938\n",
      "Epoch 00019: loss did not improve from 0.26957\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.3103 - accuracy: 0.8946\n",
      "Epoch 20/200\n",
      "650/664 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9292\n",
      "Epoch 00020: loss improved from 0.26957 to 0.21871, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2187 - accuracy: 0.9307\n",
      "Epoch 21/200\n",
      "645/664 [============================>.] - ETA: 0s - loss: 0.2871 - accuracy: 0.9054\n",
      "Epoch 00021: loss did not improve from 0.21871\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2815 - accuracy: 0.9066\n",
      "Epoch 22/200\n",
      "635/664 [===========================>..] - ETA: 0s - loss: 0.2080 - accuracy: 0.9244\n",
      "Epoch 00022: loss improved from 0.21871 to 0.20857, saving model to ./models/ramses_kpi/tmp/checkpoint\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2086 - accuracy: 0.9262\n",
      "Epoch 23/200\n",
      "660/664 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9288\n",
      "Epoch 00023: loss did not improve from 0.20857\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2112 - accuracy: 0.9277\n",
      "Epoch 24/200\n",
      "655/664 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.9069\n",
      "Epoch 00024: loss did not improve from 0.20857\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2544 - accuracy: 0.9066\n",
      "Epoch 25/200\n",
      "655/664 [============================>.] - ETA: 0s - loss: 0.2746 - accuracy: 0.9053\n",
      "Epoch 00025: loss did not improve from 0.20857\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2773 - accuracy: 0.9036\n",
      "Epoch 26/200\n",
      "645/664 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9209\n",
      "Epoch 00026: loss did not improve from 0.20857\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2251 - accuracy: 0.9217\n",
      "Epoch 27/200\n",
      "660/664 [============================>.] - ETA: 0s - loss: 0.2334 - accuracy: 0.9197\n",
      "Epoch 00027: loss did not improve from 0.20857\n",
      "664/664 [==============================] - 1s 2ms/sample - loss: 0.2322 - accuracy: 0.9202\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = [\"'s\"]\n",
    "ignore_words.extend(string.punctuation)\n",
    "\n",
    "try:\n",
    "    with open('data.pickle', 'rb') as file:\n",
    "        words, classes, documents = pickle.load(file)\n",
    "except:\n",
    "    # loop through each sentence in our intents pattern\n",
    "    for intent in data['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            # tokenize each word in the sentence\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds) # add to word list\n",
    "            documents.append((wrds, intent['tag'])) # add to documents in our corpus\n",
    "\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag']) # add to our class list\n",
    "\n",
    "    # stem and lower each word and remove duplicates\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    classes = sorted(list(set(classes))) # sort classes\n",
    "\n",
    "    print(f\"{len(documents)} documents\\n{len(classes)} classes\\n{len(words)} unique stemmed words\")\n",
    "\n",
    "#     with open('data.pickle', 'wb') as file:\n",
    "#         pickle.dump((words, classes, documents), file)\n",
    "\n",
    "\n",
    "# create our training data\n",
    "training = []\n",
    "output_empty = [0 for _ in range(len(classes))]\n",
    "\n",
    "for doc in documents: # bag of words\n",
    "    bag = []\n",
    "    s_words = [stemmer.stem(w.lower()) for w in doc[0] if w not in ignore_words]\n",
    "    \n",
    "    for w in words:\n",
    "        bag.append(1) if w in s_words else bag.append(0)\n",
    "        \n",
    "    output_row = output_empty[:]\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "# shuffle our features and turn intp np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test list.\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n\",model.summary(), \"\\n\")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1, callbacks=my_callbacks)\n",
    "model.load_weights(\"./models/ramses_kpi/tmp/checkpoint\")\n",
    "model.save(\"./models/ramses_kpi/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what my revenue for last month?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\"}), <class 'NoneType'>\n",
      "[1.8831724e-08 2.3302056e-09 4.2845314e-09 3.1444601e-05 2.4185078e-11\n",
      " 5.1414616e-11 3.2468556e-08 8.4221987e-12 2.1041441e-11 1.4667868e-08\n",
      " 3.7320362e-08 3.9492947e-09 3.1837234e-07 2.2074962e-06 9.9993002e-01\n",
      " 7.7775966e-11 1.5825063e-11 3.4911800e-06 6.7633469e-13 3.2908902e-13\n",
      " 3.8372523e-06 5.3877955e-07 7.7589011e-14 4.2667591e-14 2.3066610e-07\n",
      " 9.7833397e-10 1.2783601e-10 4.7022436e-10 2.7936963e-05]\n",
      "('last_month_revenue', '0.99993')\n",
      "\n",
      " _h1 _f1 Here is your revenue for last month. h1_ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    s_words = nltk.word_tokenize(sentence)\n",
    "    s_words = [stemmer.stem(w.lower()) for w in s_words if w not in ignore_words]\n",
    "    \n",
    "    return s_words\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    s_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    \n",
    "    for s in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                \n",
    "                if show_details:\n",
    "                    print(f\"found in bag: {s}\")\n",
    "                    \n",
    "    return np.array(bag)\n",
    "\n",
    "def classify_local(sentence):\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    \n",
    "    input_data = pd.DataFrame([bow(sentence, words, show_details=False)], dtype=float, index=['input'])\n",
    "    results = model.predict([input_data])[0]\n",
    "    \n",
    "    #filter out prediction below a threshold, and provide intent index\n",
    "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "    \n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=False)\n",
    "    \n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], str(r[1])))\n",
    "    \n",
    "    return return_list\n",
    "    \n",
    "while True:\n",
    "    inp = input(\"You: \")\n",
    "    \n",
    "    if inp == 'quit' or inp == 'stop' or inp == 'q':\n",
    "        break\n",
    "        \n",
    "    results = classify_local(inp)[0]\n",
    "    print(results)\n",
    "    \n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == results[0]:\n",
    "            responses = intent['responses']\n",
    "    \n",
    "    print(\"\\n\", random.choice(responses), \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = load_model(filepath)\n",
    "# inp = input(\"You: \")\n",
    "# input_data = pd.DataFrame([bow(inp, words, show_details=False)], dtype=float, index=['input'])\n",
    "# assert_allclose(model.predict(input_data), new_model.predict(input_data), 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloaded_obj = load_model(\"models/ramses_kpi/model.h5\")\n",
    "# reloaded_obj.load_weights(\"./models/ramses_kpi/tmp/checkpoint\")\n",
    "# reloaded_obj.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"./models/ramses_kpi/model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.normpath(filename + os.sep + os.pardir) + \"/tmp/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
