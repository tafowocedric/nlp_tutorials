{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM, Bidirectional, Reshape\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kpi_identifier.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"tmp/checkpoint\"\n",
    "\n",
    "my_callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10),\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.01, patience=10, min_lr=0.001, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639 documents\n",
      "12 classes\n",
      "163 unique stemmed words\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = [\"'s\"]\n",
    "ignore_words.extend(string.punctuation)\n",
    "\n",
    "# try:\n",
    "#     with open('data.pickle', 'rb') as file:\n",
    "#         words, classes, documents = pickle.load(file)\n",
    "# except:\n",
    "# loop through each sentence in our intents pattern\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds) # add to word list\n",
    "        documents.append((wrds, intent['tag'])) # add to documents in our corpus\n",
    "\n",
    "    if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag']) # add to our class list\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "classes = sorted(list(set(classes))) # sort classes\n",
    "\n",
    "print(f\"{len(documents)} documents\\n{len(classes)} classes\\n{len(words)} unique stemmed words\")\n",
    "\n",
    "with open('data.pickle', 'wb') as file:\n",
    "    pickle.dump((words, classes, documents), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.19.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(639, 1, 163) (639, 12)\n",
      "\n",
      "============================= Load existing model weights =============================\n",
      "\n",
      "Epoch 1/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00001: loss improved from inf to 0.00194, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0019 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9984\n",
      "Epoch 00002: loss did not improve from 0.00194\n",
      "639/639 [==============================] - 5s 9ms/step - loss: 0.0047 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00003: loss did not improve from 0.00194\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0024 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00004: loss improved from 0.00194 to 0.00162, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0016 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00005: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00006: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9984\n",
      "Epoch 00007: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0047 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00008: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0033 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9984\n",
      "Epoch 00009: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0039 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9968\n",
      "Epoch 00010: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0081 - accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9984\n",
      "Epoch 00011: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0058 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00012: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0036 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00013: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0020 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00015: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0028 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00016: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0021 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00017: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0020 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00018: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0019 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00019: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0021 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00020: loss did not improve from 0.00162\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00021: loss improved from 0.00162 to 0.00101, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00022: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0024 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 00023: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0030 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00024: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00025: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9969\n",
      "Epoch 00026: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0042 - accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9984\n",
      "Epoch 00027: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0040 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9984\n",
      "Epoch 00028: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0028 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00029: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9984\n",
      "Epoch 00030: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0022 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0025 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00032: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9968\n",
      "Epoch 00033: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0057 - accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00034: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00035: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0019 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9984\n",
      "Epoch 00036: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0041 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00037: loss did not improve from 0.00101\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 9.1579e-04 - accuracy: 1.0000\n",
      "Epoch 00038: loss improved from 0.00101 to 0.00091, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 9.1149e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00039: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00040: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00042: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 9.3222e-04 - accuracy: 1.0000\n",
      "Epoch 00043: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 9.3078e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00044: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 9.9895e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00045: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9968\n",
      "Epoch 00046: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0105 - accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9984\n",
      "Epoch 00047: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0042 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9969\n",
      "Epoch 00048: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 4s 7ms/step - loss: 0.0041 - accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9984\n",
      "Epoch 00049: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0040 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00050: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0035 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 00052: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0033 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00053: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00054: loss did not improve from 0.00091\n",
      "639/639 [==============================] - 5s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 8.7450e-04 - accuracy: 1.0000\n",
      "Epoch 00055: loss improved from 0.00091 to 0.00087, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 8.6630e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00056: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9984\n",
      "Epoch 00057: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0035 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9984\n",
      "Epoch 00058: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0028 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00059: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9984\n",
      "Epoch 00060: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 5s 9ms/step - loss: 0.0033 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9984\n",
      "Epoch 00061: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0054 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 00062: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0037 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9984\n",
      "Epoch 00063: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0024 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00064: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00066: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 9.7322e-04 - accuracy: 1.0000\n",
      "Epoch 00067: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 9.6728e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 00068: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0027 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00069: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00070: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00071: loss did not improve from 0.00087\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 6.4005e-04 - accuracy: 1.0000\n",
      "Epoch 00072: loss improved from 0.00087 to 0.00064, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 6.4005e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 6.5262e-04 - accuracy: 1.0000\n",
      "Epoch 00073: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 6.5163e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 8.9541e-04 - accuracy: 1.0000\n",
      "Epoch 00074: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 8.8987e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9984\n",
      "Epoch 00075: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0025 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9984\n",
      "Epoch 00076: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0037 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 6.4336e-04 - accuracy: 1.0000\n",
      "Epoch 00077: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 6.4336e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00078: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00079: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00080: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00081: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0020 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9984\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.00064\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0042 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 4.1658e-04 - accuracy: 1.0000\n",
      "Epoch 00083: loss improved from 0.00064 to 0.00042, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 4.1658e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9984\n",
      "Epoch 00084: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0056 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 00085: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0029 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00086: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00087: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0019 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00088: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0024 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00089: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 7.5377e-04 - accuracy: 1.0000\n",
      "Epoch 00090: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 7.5164e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 7.6282e-04 - accuracy: 1.0000\n",
      "Epoch 00091: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 12ms/step - loss: 7.6043e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00092: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0017 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0025 - accuracy: 0.9984\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0024 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00094: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00095: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0019 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9984\n",
      "Epoch 00096: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0063 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00097: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0024 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9984\n",
      "Epoch 00098: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0032 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00099: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00100: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0016 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 8.3679e-04 - accuracy: 1.0000\n",
      "Epoch 00101: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 8.3570e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 6.7973e-04 - accuracy: 1.0000\n",
      "Epoch 00102: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 6.7548e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00103: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 8.6643e-04 - accuracy: 1.0000\n",
      "Epoch 00104: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 8.5855e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9984\n",
      "Epoch 00105: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0053 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00106: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0017 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00107: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00108: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00109: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0013 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 8.0801e-04 - accuracy: 1.0000\n",
      "Epoch 00110: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 8.0675e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00111: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 9.6888e-04 - accuracy: 1.0000\n",
      "Epoch 00112: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 9.6446e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 0.9984\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0021 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 6.4938e-04 - accuracy: 1.0000\n",
      "Epoch 00114: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 6.4456e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00115: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00116: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00117: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 6.5052e-04 - accuracy: 1.0000\n",
      "Epoch 00118: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 6.6600e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 00119: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 5s 9ms/step - loss: 0.0021 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 7.1404e-04 - accuracy: 1.0000\n",
      "Epoch 00120: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 7.0957e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00121: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00122: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 7.4457e-04 - accuracy: 1.0000\n",
      "Epoch 00124: loss did not improve from 0.00042\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 7.4225e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 3.4497e-04 - accuracy: 1.0000\n",
      "Epoch 00125: loss improved from 0.00042 to 0.00035, saving model to tmp/checkpoint\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 3.4581e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 6.5829e-04 - accuracy: 1.0000\n",
      "Epoch 00126: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 13ms/step - loss: 6.5626e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000    \n",
      "Epoch 00127: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00128: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 5.5042e-04 - accuracy: 1.0000\n",
      "Epoch 00129: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 5.4956e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00130: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 8.4559e-04 - accuracy: 1.0000\n",
      "Epoch 00131: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 8.4427e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 8.9772e-04 - accuracy: 1.0000\n",
      "Epoch 00132: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 8.9756e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9984\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00133: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0033 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 7.2579e-04 - accuracy: 1.0000\n",
      "Epoch 00134: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 7.2263e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 9.6906e-04 - accuracy: 1.0000\n",
      "Epoch 00135: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 9.6149e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00136: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00137: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 00138: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0016 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 6.6782e-04 - accuracy: 1.0000\n",
      "Epoch 00139: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 6.6609e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 9.6262e-04 - accuracy: 1.0000\n",
      "Epoch 00140: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 9.6282e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00141: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0013 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 5.0088e-04 - accuracy: 1.0000\n",
      "Epoch 00142: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 13ms/step - loss: 5.0914e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 5.4171e-04 - accuracy: 1.0000\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 5.4001e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 6.1072e-04 - accuracy: 1.0000\n",
      "Epoch 00144: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 12ms/step - loss: 6.1556e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 6.0063e-04 - accuracy: 1.0000\n",
      "Epoch 00145: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 6.0713e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00146: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 9.9755e-04 - accuracy: 1.0000\n",
      "Epoch 00147: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 9.9599e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9984\n",
      "Epoch 00148: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0070 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 00149: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0028 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 8.9955e-04 - accuracy: 1.0000\n",
      "Epoch 00150: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 8.9674e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00151: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00152: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0013 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "631/639 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00153: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0020 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 8.7753e-04 - accuracy: 1.0000\n",
      "Epoch 00154: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 8.7330e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9984\n",
      "Epoch 00155: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0030 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 8.3833e-04 - accuracy: 1.0000\n",
      "Epoch 00156: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 12ms/step - loss: 8.3714e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 8.2849e-04 - accuracy: 1.0000\n",
      "Epoch 00157: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 8.2720e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00158: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9969\n",
      "Epoch 00159: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0049 - accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 00160: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0019 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9969\n",
      "Epoch 00161: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0049 - accuracy: 0.9969 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00162: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 00164: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 10ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9984\n",
      "Epoch 00165: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0032 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 00166: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0023 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 6.9068e-04 - accuracy: 1.0000\n",
      "Epoch 00167: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 6.8743e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "633/639 [============================>.] - ETA: 0s - loss: 6.1792e-04 - accuracy: 1.0000\n",
      "Epoch 00168: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 6.5185e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "631/639 [============================>.] - ETA: 0s - loss: 4.7468e-04 - accuracy: 1.0000\n",
      "Epoch 00169: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 4.7007e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 4.9038e-04 - accuracy: 1.0000\n",
      "Epoch 00170: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 4.9038e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 6.1404e-04 - accuracy: 1.0000\n",
      "Epoch 00171: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 6.1570e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 6.8638e-04 - accuracy: 1.0000\n",
      "Epoch 00172: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 6.8455e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 8.3461e-04 - accuracy: 1.0000\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 8.3331e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00174: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 6.2866e-04 - accuracy: 1.0000\n",
      "Epoch 00175: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 6.2770e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 8.4317e-04 - accuracy: 1.0000\n",
      "Epoch 00176: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 8.4317e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00177: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 9s 15ms/step - loss: 0.0015 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9937\n",
      "Epoch 00178: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0081 - accuracy: 0.9937 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 00179: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 13ms/step - loss: 0.0024 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 00180: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00181: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 9s 14ms/step - loss: 0.0020 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 9.2379e-04 - accuracy: 1.0000\n",
      "Epoch 00182: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 9.2379e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "632/639 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00183: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0011 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 9.8678e-04 - accuracy: 1.0000\n",
      "Epoch 00184: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 13ms/step - loss: 9.8678e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9984\n",
      "Epoch 00185: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0032 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00186: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 0.0018 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "638/639 [============================>.] - ETA: 0s - loss: 6.0939e-04 - accuracy: 1.0000\n",
      "Epoch 00187: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 8.7442e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 5.8232e-04 - accuracy: 1.0000\n",
      "Epoch 00188: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 6.9792e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00189: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 00190: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0014 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 8.6682e-04 - accuracy: 1.0000\n",
      "Epoch 00191: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 8.6287e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 4.6677e-04 - accuracy: 1.0000\n",
      "Epoch 00192: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 7s 11ms/step - loss: 4.6616e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "635/639 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00193: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 12ms/step - loss: 0.0022 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 8.8224e-04 - accuracy: 1.0000\n",
      "Epoch 00194: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 9s 13ms/step - loss: 8.7989e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 7.7762e-04 - accuracy: 1.0000\n",
      "Epoch 00195: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 8s 13ms/step - loss: 7.7226e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 00196: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "636/639 [============================>.] - ETA: 0s - loss: 6.2305e-04 - accuracy: 1.0000\n",
      "Epoch 00197: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 6.2013e-04 - accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "639/639 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9984\n",
      "Epoch 00198: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 6s 9ms/step - loss: 0.0022 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "637/639 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9984\n",
      "Epoch 00199: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 0.0039 - accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "634/639 [============================>.] - ETA: 0s - loss: 7.1619e-04 - accuracy: 1.0000\n",
      "Epoch 00200: loss did not improve from 0.00035\n",
      "639/639 [==============================] - 5s 8ms/step - loss: 7.1064e-04 - accuracy: 1.0000 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6bcc74b490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output_empty = [0 for _ in range(len(classes))]\n",
    "\n",
    "for doc in documents: # bag of words\n",
    "    bag = []\n",
    "    s_words = [stemmer.stem(w.lower()) for w in doc[0] if w not in ignore_words]\n",
    "    \n",
    "    for w in words:\n",
    "        bag.append(1) if w in s_words else bag.append(0)\n",
    "        \n",
    "    output_row = output_empty[:]\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n",
    "    \n",
    "# shuffle our features and turn intp np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test list.\n",
    "train_x = tf.cast(np.array(list(training[:, 0])), tf.float32)\n",
    "train_y = tf.cast(np.array(list(training[:, 1])), tf.float32)\n",
    "\n",
    "train_x = tf.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "print(f\"{train_x.shape} {train_y.shape}\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=train_x.shape[2], input_shape=(1, train_x.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(train_y.shape[1], activation=\"softmax\"))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "if not tf.io.gfile.exists(filepath):\n",
    "    print(\"\\n============================= Create a new model =============================\\n\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "else:\n",
    "    print(\"\\n============================= Load existing model weights =============================\\n\")\n",
    "    model = tf.keras.models.load_model(\"kpi_identifier.hdf5\")\n",
    "    model.load_weights(filepath)\n",
    "    \n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_x, train_y, epochs=200, batch_size=1, verbose=1, callbacks=my_callbacks)\n",
    "# model.load_weights(\"./models/ramses_kpi/tmp/checkpoint\")\n",
    "# model.save(\"./models/ramses_kpi/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    s_words = nltk.word_tokenize(sentence)\n",
    "    s_words = [stemmer.stem(w.lower()) for w in s_words if w not in ignore_words]\n",
    "    \n",
    "    return s_words\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    s_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    \n",
    "    for s in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                \n",
    "                if show_details:\n",
    "                    print(f\"found in bag: {s}\")\n",
    "                    \n",
    "    return np.array(bag)\n",
    "\n",
    "def classify_local(sentence):\n",
    "    ERROR_THRESHOLD = 0.6\n",
    "    \n",
    "    input_data = pd.DataFrame([bow(sentence, words, show_details=False)], dtype=float, index=['input'])\n",
    "    input_data = input_data.values.reshape(-1, 1, input_data.shape[1])\n",
    "        \n",
    "    results = model.predict([input_data])[0]\n",
    "    \n",
    "    #filter out prediction below a threshold, and provide intent index\n",
    "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "    \n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=False)\n",
    "    \n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], str(r[1])))\n",
    "        \n",
    "#     print(return_list)\n",
    "    return return_list\n",
    "    \n",
    "# while True:\n",
    "#     inp = input(\"You: \")\n",
    "\n",
    "#     if inp == 'quit' or inp == 'stop' or inp == 'q':\n",
    "#         break\n",
    "\n",
    "#     try:\n",
    "#         results = classify_local(inp)[0]\n",
    "        \n",
    "#         for intent in data['intents']:\n",
    "#             if intent['tag'] == results[0]:\n",
    "#                 responses = results[0], inp\n",
    "\n",
    "#         print(responses)\n",
    "        \n",
    "#     except:\n",
    "#         print(\"Don't understand your query please use a different term\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "model.save(\"kpi_identifier.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KpiIdentifier:\n",
    "    def __init__(self, data_path, model_path, json_data_path):\n",
    "        try:\n",
    "            with open(data_path, 'rb') as file:\n",
    "                self.words, self.classes, self.documents = pickle.load(file)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise FileNotFoundError(f\"{data_path} doesn't exist\")\n",
    "            \n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise FileNotFoundError(f\"{model_path} doesn't exist\")\n",
    "            \n",
    "        try:\n",
    "            with open(json_data_path, \"rb\") as file:\n",
    "                self.data = json.load(file)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise FileNotFoundError(f\"{json_data_path} doesn't exist\")\n",
    "           \n",
    "    @property\n",
    "    def ignore_words(self):\n",
    "        return [\"'s\"] + [s for s in string.punctuation]\n",
    "    \n",
    "    def clean_up_sentence(self, sentence):\n",
    "        s_words = nltk.word_tokenize(sentence)\n",
    "        s_words = [stemmer.stem(w.lower()) for w in s_words if w not in self.ignore_words]\n",
    "\n",
    "        return s_words\n",
    "\n",
    "    def bow(self, sentence, words, show_details=True):\n",
    "        s_words = self.clean_up_sentence(sentence)\n",
    "\n",
    "        bag = [0 for _ in range(len(words))]\n",
    "\n",
    "        for s in s_words:\n",
    "            for i, w in enumerate(words):\n",
    "                if w == s:\n",
    "                    bag[i] = 1\n",
    "\n",
    "                    if show_details:\n",
    "                        print(f\"found in bag: {s}\")\n",
    "\n",
    "        return np.array(bag)\n",
    "\n",
    "    def classify_local(self, sentence):\n",
    "        ERROR_THRESHOLD = 0.6\n",
    "\n",
    "        input_data = pd.DataFrame([self.bow(sentence, self.words, show_details=False)], dtype=float, index=['input'])\n",
    "        input_data = tf.cast(input_data.values.reshape(-1, 1, input_data.shape[1]), tf.float32)\n",
    "        \n",
    "        results = self.model.predict(input_data)[0]\n",
    "\n",
    "        #filter out prediction below a threshold, and provide intent index\n",
    "        results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "\n",
    "        # sort by strength of probability\n",
    "        results.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "        return_list = []\n",
    "        for r in results:\n",
    "            return_list.append((self.classes[r[0]], str(r[1])))\n",
    "                \n",
    "        print(return_list)\n",
    "        return return_list\n",
    "\n",
    "    def run(self, text):\n",
    "        try:\n",
    "            results = self.classify_local(text)[0]\n",
    "            \n",
    "            for intent in self.data[\"intents\"]:\n",
    "                if intent[\"tag\"] == results[0]: \n",
    "                    for query in text.split():\n",
    "                        for val in intent[\"validate\"]:\n",
    "                            if stemmer.stem(nltk.word_tokenize(query)[0]) == stemmer.stem(val): return results[0], text\n",
    "                            elif results[0] == \"definition\" and re.search(r\"\"\"(?:is\\s)\\w{3,}\"\"\", text): return results[0], text\n",
    "                    return None\n",
    "                \n",
    "        except IndexError:\n",
    "            return None\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    \n",
    "# identifier = KpiIdentifier(data_path=\"data.pickle\", model_path=\"kpi_identifier.hdf5\", json_data_path=\"kpi_identifier.json\")\n",
    "# identifier.run(text=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Text:  how are we doing on invoice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('invoice', '0.9999963')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('invoice', 'how are we doing on invoice')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Text:  what is my revenue for today\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('revenue', '1.0')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('revenue', 'what is my revenue for today')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter Text:  quit\n"
     ]
    }
   ],
   "source": [
    "identifier = KpiIdentifier(data_path=\"data.pickle\", model_path=\"kpi_identifier.hdf5\", json_data_path=\"kpi_identifier.json\")\n",
    "text = input(\"Enter Text: \")\n",
    "\n",
    "while True:\n",
    "    if text in [\"quit\", \"exit\", \"stop\", \"q\", \"s\", \"e\"]:\n",
    "        break\n",
    "    \n",
    "    display(identifier.run(text=text))\n",
    "    text = input(\"\\nEnter Text: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
