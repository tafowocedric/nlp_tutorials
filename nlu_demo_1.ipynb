{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "import random\n",
    "import contractions\n",
    "import langdetect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from selenium import webdriver\n",
    "import unicodedata\n",
    "import arrow\n",
    "from pattern.en import number as num_string_to_digit\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">List of Constants</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    IGNORED_WORDS = [\"what\", \"who\", \"when\", \"how\", \"which\", \"whom\", \"where\"]\n",
    "    LANGUAGE_SUPPORTED = [\"en\"]\n",
    "        \n",
    "    # nltk chunk regexp\n",
    "    NOUN_PHRASE_REGX_PATTERN = {\"label\": \"Noun Phrase\", \"include\": \"{<DT>*<NN.*|JJ.*>*<NN.*>}\", \"exclude\": \"}<VB.*|JJ.*|RB.*|IN|DT>+{\", \"tag\": \"NP\"}\n",
    "    WH_PRO_REGX_PATTERN = {\"label\": \"Wh-pronoun\", \"example\": [\"who\", \"which\", \"whome\", \"whose\", \"what\"], \"include\": \"{<WP.*>}\", \"tag\": \"WH_PRO\", \"valid_words\": [\"what\"]}\n",
    "    WH_DET_REGX_PATTERN = {\"label\": \"Wh-determiner\", \"example\": [\"what\", \"which\", \"whose\"], \"include\": \"{<WDT>}\", \"tag\": \"WH_DET\", \"valid_words\": [\"what\"]}\n",
    "    WH_ADV_REGX_PATTERN = {\"label\": \"Wh-adverb\", \"example\": [\"how\", \"why\", \"where\", \"when\"], \"include\": \"{<WRB>}\", \"tag\": \"WH_ADV\", \"valid_words\": [\"how\"]}\n",
    "    PERIOD_REGX_PATTERN = {\"label\": \"Time Periods\", \"include\": \"{<DT>*<JJ.>*<CD>*<NN.>}\", \"tag\": \"PERIOD\"}\n",
    "    \n",
    "    # Revenue chunk extractors WDT\n",
    "    KPI_EXACT_REGX_PATTERN = {\"label\": \"KPI\", \"include\": [\"{<WP.*>?<VB.*>*<PRP.><NN.*>}\", \"{<WP.*>*<VB.*>*<PRP.><JJ.*>*<NN.*><NN.*>?}\"], \"tag\": \"KPI_EXACT\"}\n",
    "    \n",
    "    # Google search noun subjects\n",
    "    SEARCH_REGX_PATTERN = {\"label\": \"Google Search\", \"include\": [\"{<WP.*|WDT|WRB><VB.*><JJ.*>*<NN.*><IN>?<NN.*>?}\", \"{<IN|UH>*<VB.*><RP>*<IN|UH>?<DT>?<PRP>?<JJ.*>*<IN>?<NN.*><IN>?<NN.*>?}\"], \"exclude\": \"}<WP.*|WRB|WDT|VB.*|JJ.*|RB.*|RP|PRP|IN|UH|DT>+{\", \"tag\": \"DEFN\"}\n",
    "    \n",
    "    # Defination chunk extraction\n",
    "    DEFINATION_REGX_PATTERN = {\"label\": \"Definations or search\", \"include\": []}\n",
    "    \n",
    "    CHUNK_TAGS = [NOUN_PHRASE_REGX_PATTERN.get(\"tag\"), WH_PRO_REGX_PATTERN.get(\"tag\"), WH_ADV_REGX_PATTERN.get(\"tag\"), PERIOD_REGX_PATTERN.get(\"tag\"), KPI_EXACT_REGX_PATTERN.get(\"tag\")]\n",
    "    \n",
    "    ## Default Ramses Response\n",
    "    DEFAULT_RESPONSE = [\"sorry don't understand you. Please try saying this in another way.\"]\n",
    "    \n",
    "    # Valid KPI Types\n",
    "    KPI_TYPES = [\"revenue\", \"expense\", \"refund\", \"income\", \"refund\", \"earning\"]\n",
    "    KPI_TYPE_DICT = {\"revenue\": \"REVENUE\", \"expense\": \"EXPENSE\", \"refund\": \"REFUNDS\"}\n",
    "    \n",
    "    ## Error Responses\n",
    "    LANGUAGE_ERROR = \"Invalid language detected.\"\n",
    "    \n",
    "    ## Corpus language tenses\n",
    "    TENSE = {\"past\": \"past_tense\", \"future\": \"future_tense\"}\n",
    "    PAST_TENSE = [\"past\", \"previous\", \"last\", \"ago\"]\n",
    "    \n",
    "    ## Generals\n",
    "    BASE_PERIODS = [\"day\", \"week\", \"month\", \"year\"]\n",
    "    IMMEDIATE_PERIODS = [\"yesterday\", \"today\", \"tomorrow\"]\n",
    "    BASE_PERIOD_ERROR = f\"Invalid period. valid periods are: {BASE_PERIODS}\"\n",
    "    SERVER_ERROR = \"Server Error\"\n",
    "    ANALYSIS_SCOPE = {\"detail\": \"DETAIL\", \"exact\": \"EXACT\"}\n",
    "    SENTENCE_TYPES = {\"simple\": \"SIMPLE_SENTENCE\", \"compound\": \"COMPOUND_SENTENCE\", \"complex\": \"COMPLEX_SENTENCE\", \"compound_complex\": \"COMPOUND_COMPLEX_SENTENCE\", \"phrase\": \"PHRASE\"}\n",
    "    \n",
    "    # Context model constants\n",
    "    CONTEXT_MODEL_PATH = \"./data/kpi_identifier.hdf5\"\n",
    "    CONTEXT_MODEL_DATA_PATH = \"./data/data.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Exceptions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Model initializers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyDoc:\n",
    "    \"\"\" This is a singleton class implementation \"\"\"\n",
    "    \n",
    "    __nlp_library = \"en_core_web_lg\"\n",
    "    __nlp_instance = spacy.load(__nlp_library)\n",
    "\n",
    "    @staticmethod \n",
    "    def getInstance(doc: str):\n",
    "        \"\"\" Static access method. \"\"\"\n",
    "        \n",
    "        if SpacyDoc.__nlp_instance == None:\n",
    "            SpacyDoc()\n",
    "            \n",
    "        return SpacyDoc.__nlp_instance(doc)\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Virtually private constructor. \"\"\"\n",
    "        \n",
    "        if SpacyDoc.__nlp_instance != None:\n",
    "            raise Exception(\"This class is a singleton!\")\n",
    "            \n",
    "        else:\n",
    "            SpacyDoc.__nlp_instance = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Subject | Object & Predicate extraction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPO:\n",
    "    \"\"\"\n",
    "        This class will extract the following: SPO => subject, predicate, object\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, doc: str):\n",
    "        self.__doc = SpacyDoc.getInstance(doc=doc)\n",
    "    \n",
    "    \n",
    "    def __get_entities(self):\n",
    "        ## chunk 1\n",
    "        entity_1 = \"\"\n",
    "        entity_2 = \"\"\n",
    "\n",
    "        prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "        prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        \n",
    "        for tok in self.__doc:\n",
    "            ## chunk 2\n",
    "            # if token is a punctuation mark then move on to the next token\n",
    "            if tok.dep_ != \"punct\":\n",
    "                # check: token is a compound word or not\n",
    "                if tok.dep_ == \"compound\":\n",
    "                    prefix = tok.text\n",
    "                    # if the previous word was also a 'compound' then add the current word to it\n",
    "                    if prv_tok_dep == \"compound\":\n",
    "                        prefix = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "                # check: token is a modifier or not\n",
    "                if tok.dep_.endswith(\"mod\") == True:\n",
    "                    modifier = tok.text\n",
    "                    # if the previous word was also a 'compound' then add the current word to it\n",
    "                    if prv_tok_dep == \"compound\":\n",
    "                        modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "                ## chunk 3\n",
    "                if tok.dep_.find(\"subj\") == True:\n",
    "                    entity_1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                    prefix = \"\"\n",
    "                    modifier = \"\"\n",
    "                    prv_tok_dep = \"\"\n",
    "                    prv_tok_text = \"\"      \n",
    "\n",
    "                ## chunk 4\n",
    "                if tok.dep_.find(\"obj\") == True:\n",
    "                    entity_2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "\n",
    "                ## chunk 5  \n",
    "                # update variables\n",
    "                prv_tok_dep = tok.dep_\n",
    "                prv_tok_text = tok.text\n",
    "        \n",
    "        \n",
    "        if entity_1:\n",
    "            entity_1.strip()\n",
    "        else:\n",
    "            entity_1 = None\n",
    "            \n",
    "        if entity_2:\n",
    "            entity_2.strip()\n",
    "        else:\n",
    "            entity_2 = None\n",
    "            \n",
    "        return entity_1, entity_2\n",
    "    \n",
    "    \n",
    "    def __get_predicate(self):\n",
    "        matcher = spacy.matcher.Matcher(self.__doc.vocab)\n",
    "        \n",
    "        #define the pattern \n",
    "        pattern = [ {'DEP':'ROOT'},\n",
    "                    {'DEP':'prep','OP':\"?\"},\n",
    "                    {'DEP':'agent','OP':\"?\"},  \n",
    "                    {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "        matcher.add(\"PREDICATE_MATCH\", None, pattern) \n",
    "        \n",
    "        matches = matcher(self.__doc)\n",
    "        k = len(matches) - 1\n",
    "        \n",
    "        span = self.__doc[matches[k][1]:matches[k][2]]\n",
    "        \n",
    "        return span.text\n",
    "    \n",
    "    \n",
    "    def main(self):\n",
    "        predicate = self.__get_predicate()\n",
    "        subj, obj = self.__get_entities()\n",
    "        \n",
    "        if subj is None and obj is None:\n",
    "            try:\n",
    "                subj = [noun.text for noun in self.__doc.noun_chunks][0]\n",
    "                \n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "        return subj, predicate, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('revenue', 'revenue', None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPO(\"revenue\").main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Sentence Classification</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_classifier(doc: str):\n",
    "    doc = SpacyDoc.getInstance(doc=doc)\n",
    "    subjects = []\n",
    "    objects = []\n",
    "    verbs = []\n",
    "    dep_clause = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.dep_ != \"punct\":\n",
    "            if token.dep_.find(\"subj\") == True:\n",
    "                subjects.append(token.text)\n",
    "            \n",
    "            if token.dep_.find(\"obj\") == True:\n",
    "                objects.append(token.text)\n",
    "            \n",
    "            if \"VB\" in token.tag_:\n",
    "                verbs.append(token.text)\n",
    "            \n",
    "            if token.pos_ == \"SCONJ\" or token.dep_ == \"mark\":\n",
    "                dep_clause.append(token.text)\n",
    "    \n",
    "    if len(subjects) == 1 and len(verbs) >= 1:\n",
    "        return Constants.SENTENCE_TYPES.get(\"simple\")\n",
    "    \n",
    "    elif len(subjects) >= 2 and not dep_clause and len(verbs) >= 2:\n",
    "        return Constants.SENTENCE_TYPES.get(\"compound\")\n",
    "    \n",
    "    elif dep_clause:\n",
    "        return random.choice([Constants.SENTENCE_TYPES.get(\"complex\"), Constants.SENTENCE_TYPES.get(\"compound_complex\")])\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return Constants.SENTENCE_TYPES.get(\"phrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PHRASE'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_classifier(\"Need some insights on revenue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Corpus Normalization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusNormalization:\n",
    "    def __init__(self, corpus: str):\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    def normalize(self, contraction_expansion: bool = True,\n",
    "                     accented_char_removal: bool = True, text_lower_case: bool = True, \n",
    "                     text_lemmatization: bool = False, special_char_removal: bool = True, \n",
    "                     stopword_removal: bool = True, remove_digits: bool = False) -> str:\n",
    "        \n",
    "        # validate corpus language\n",
    "        CorpusNormalization.__language_validator(corpus=self.corpus)\n",
    "\n",
    "        normalized_corpus = []\n",
    "        corpus = CorpusNormalization.__nltk_sent_tokenize(corpus=self.corpus)\n",
    "\n",
    "        # normalize each document in the corpus\n",
    "        for doc in corpus:\n",
    "            # strip spaces\n",
    "            doc = doc.strip()\n",
    "\n",
    "            # remove accented characters\n",
    "            if accented_char_removal:\n",
    "                doc = CorpusNormalization.__remove_accented_chars(doc=doc)\n",
    "\n",
    "            # expand contractions    \n",
    "            if contraction_expansion:\n",
    "                doc = CorpusNormalization.__expand_contractions(doc=doc)\n",
    "\n",
    "            # lowercase the text    \n",
    "            if text_lower_case:\n",
    "                doc = doc.lower()\n",
    "            \n",
    "            # remove extra newlines\n",
    "            doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "            \n",
    "            # remove repeated characters and correcting words\n",
    "            doc = CorpusNormalization.__remove_repeated_characters(doc=doc)\n",
    "            doc = CorpusNormalization.__correct_text(doc=doc)\n",
    "\n",
    "            # lemmatize text\n",
    "            if text_lemmatization:\n",
    "                doc = CorpusNormalization.__lemmatize_text(doc=doc)\n",
    "\n",
    "            # remove special characters and\\or digits    \n",
    "            if special_char_removal:\n",
    "                # insert spaces between special characters to isolate them    \n",
    "                special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "                doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "                doc = CorpusNormalization.__remove_special_characters(doc=doc, remove_digits=remove_digits)  \n",
    "\n",
    "            # remove extra whitespace\n",
    "            doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "            # remove stopwords\n",
    "            if stopword_removal:\n",
    "                doc = CorpusNormalization.__remove_stopwords(doc=doc)\n",
    "\n",
    "            normalized_corpus.append(doc.strip())\n",
    "\n",
    "        # combining results\n",
    "        normalized_corpus = \" \".join(doc for doc in normalized_corpus)\n",
    "        normalized_corpus = normalized_corpus.strip()\n",
    "\n",
    "        return normalized_corpus\n",
    "    \n",
    "    # spacy lemmatization\n",
    "    @staticmethod\n",
    "    def __lemmatize_text(doc: str) -> str:\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        doc = \" \".join(lemmatizer.lemmatize(token) for token in doc.split())\n",
    "        \n",
    "        return doc\n",
    "    \n",
    "    # Correcting an english word\n",
    "    @staticmethod\n",
    "    def __correct_text(doc: str) -> str:\n",
    "        doc = TextBlob(doc)\n",
    "        \n",
    "        return doc.correct().__str__()\n",
    "    \n",
    "    # Removing Accented Characters\n",
    "    @staticmethod\n",
    "    def __remove_accented_chars(doc: str):\n",
    "        doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return doc\n",
    "    \n",
    "    # NLTK sentence tokenization\n",
    "    @staticmethod\n",
    "    def __nltk_sent_tokenize(corpus: str) -> list:\n",
    "        corpus = nltk.tokenize.sent_tokenize(corpus)\n",
    "        \n",
    "        return corpus\n",
    "    \n",
    "    # Removing special characters\n",
    "    @staticmethod\n",
    "    def __remove_special_characters(doc: str, remove_digits=False) -> str:\n",
    "        pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "        doc = re.sub(pattern, '', doc)\n",
    "\n",
    "        return doc\n",
    "    \n",
    "    # Removing repeating characters\n",
    "    @staticmethod\n",
    "    def __remove_repeated_characters(doc: str) -> str:\n",
    "        repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        match_substitution = r'\\1\\2\\3'\n",
    "\n",
    "        tokens = nltk.tokenize.word_tokenize(doc)\n",
    "\n",
    "        def replace(old_word):\n",
    "            if nltk.corpus.wordnet.synsets(old_word):\n",
    "                return old_word\n",
    "            new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "\n",
    "            return replace(new_word) if new_word != old_word else new_word\n",
    "\n",
    "        correct_tokens = [replace(word) for word in tokens]\n",
    "        return ' '.join(correct_tokens)\n",
    "    \n",
    "    # Removing stopwords\n",
    "    @staticmethod\n",
    "    def __remove_stopwords(doc: str) -> str:\n",
    "        stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "        response = []\n",
    "    \n",
    "        for token in nltk.tokenize.word_tokenize(doc):\n",
    "            if token not in stop_words or token in Constants.IGNORED_WORDS:\n",
    "                response.append(token)\n",
    "        \n",
    "        return \" \".join(word for word in response)\n",
    "    \n",
    "    # expand contracted document\n",
    "    @staticmethod\n",
    "    def __expand_contractions(doc: str) -> str:\n",
    "        response = []\n",
    "\n",
    "        for word_token in doc.split():\n",
    "            response.append(contractions.fix(word_token))\n",
    "\n",
    "        return \" \".join(word_token for word_token in response)\n",
    "    \n",
    "    # Convert number string to digit\n",
    "    @staticmethod\n",
    "    def __num_str_to_digit(string: str) -> int:\n",
    "        return num_string_to_digit(string)\n",
    "    \n",
    "    # language validator\n",
    "    @staticmethod\n",
    "    def __language_validator(corpus: str, using_textblob: bool = True):\n",
    "        try:\n",
    "            language = TextBlob(corpus).detect_language()\n",
    "            \n",
    "        except Exception:\n",
    "            language =  langdetect.detect(corpus)\n",
    "            \n",
    "        finally:\n",
    "            if language not in Constants.LANGUAGE_SUPPORTED:\n",
    "                raise GlobalException(Constants.LANGUAGE_ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Extracting start_date and stop_date from normalized document</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodExtractor:\n",
    "    \"\"\"\n",
    "        This is a period extractor class. it will go through the user query and identify the date ranges of our query.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: str):\n",
    "        self.__immediate_period = False\n",
    "        self.__tense = Constants.TENSE.get(\"future\")\n",
    "        self.__doc = SpacyDoc.getInstance(doc=corpus)\n",
    "        self.__doc_entities = [ent.label_ for ent in self.__doc.ents]\n",
    "        \n",
    "    def __get_query_date(self, period: str, date: str, shift_range: int):\n",
    "        date = arrow.get(date)\n",
    "        response = date\n",
    "\n",
    "        if period == Constants.BASE_PERIODS[0]:\n",
    "            response = date.shift(days=shift_range)\n",
    "\n",
    "        elif period == Constants.BASE_PERIODS[1]:\n",
    "            response = date.shift(weeks=shift_range)\n",
    "\n",
    "        elif period == Constants.BASE_PERIODS[2]:\n",
    "            response = date.shift(months=shift_range)\n",
    "\n",
    "        elif period == Constants.BASE_PERIODS[3]:\n",
    "            response = date.shift(years=shift_range)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return response\n",
    "    \n",
    "    \n",
    "    def __period_shift_range(self, period: str, shift_range: int, start_date: str = None, stop_date: str = None):\n",
    "        if not self.__immediate_period:\n",
    "            if period not in Constants.BASE_PERIODS:\n",
    "                return None, None\n",
    "#                 raise GlobalException(Constants.BASE_PERIOD_ERROR)\n",
    "\n",
    "            if start_date is None and stop_date is None:\n",
    "                return None, None\n",
    "#                 raise GlobalException(\"Date is required.\")\n",
    "            \n",
    "            if start_date is not None and stop_date is None:\n",
    "                stop_date = self.__get_query_date(period=period, date=start_date, shift_range=shift_range)\n",
    "\n",
    "            elif  stop_date is not None and start_date is None:\n",
    "                start_date = self.__get_query_date(period=period, date=stop_date, shift_range=-shift_range)\n",
    "\n",
    "            else:\n",
    "                return None, None\n",
    "#                 raise GlobalException(Constants.SERVER_ERROR)\n",
    "\n",
    "        elif self.__immediate_period:\n",
    "            date = arrow.now()\n",
    "            \n",
    "            if period == Constants.IMMEDIATE_PERIODS[0]:\n",
    "                start_date = date.shift(days=-1)\n",
    "                stop_date = date\n",
    "\n",
    "            elif period == Constants.IMMEDIATE_PERIODS[1]:\n",
    "                start_date = date\n",
    "                stop_date = date\n",
    "\n",
    "            elif period == Constants.IMMEDIATE_PERIODS[2]:\n",
    "                start_date = date\n",
    "                stop_date = date.shift(days=1)\n",
    "\n",
    "            else:\n",
    "                return None, None\n",
    "#                 raise GlobalException(Constants.SERVER_ERROR)\n",
    "\n",
    "        else:\n",
    "            return None, None\n",
    "#             raise GlobalException(Constants.SERVER_ERROR)\n",
    "\n",
    "        return arrow.get(start_date).date(), arrow.get(stop_date).date()\n",
    "    \n",
    "    \n",
    "    def __get_period_date_range(self, doc) -> int:\n",
    "        period_tags = []\n",
    "        numbers = []\n",
    "        period = None\n",
    "        start_date = None\n",
    "        stop_date = None\n",
    "        \n",
    "        for token in doc:\n",
    "            period_tags.append(token.tag_)\n",
    "            \n",
    "            if token.lemma_ in Constants.IMMEDIATE_PERIODS:\n",
    "                self.__immediate_period = True\n",
    "                start_date, stop_date = self.__period_shift_range(period=token.lemma_, shift_range=None)\n",
    "\n",
    "                return start_date, stop_date\n",
    "            \n",
    "            if token.lemma_ in Constants.BASE_PERIODS:\n",
    "                period = token.lemma_\n",
    "            \n",
    "            if token.pos_ == \"NUM\":\n",
    "                numbers.append(token.text)\n",
    "        \n",
    "        num_digit = num_string_to_digit(s = \" \".join(num for num in numbers))\n",
    "        num_digit = 1 if num_digit == 0 else num_digit\n",
    "\n",
    "        if self.__tense == Constants.TENSE.get(\"past\"):\n",
    "            stop_date = arrow.now()\n",
    "            start_date, stop_date = self.__period_shift_range(period=period, shift_range=num_digit, stop_date=stop_date, start_date=None)\n",
    "\n",
    "        elif self.__tense == Constants.TENSE.get(\"future\"):\n",
    "            start_date = arrow.now()\n",
    "            start_date, stop_date = self.__period_shift_range(period=period, shift_range=num_digit, stop_date=None, start_date=start_date)\n",
    "\n",
    "        else:\n",
    "            pass        \n",
    "\n",
    "        return start_date, stop_date\n",
    "    \n",
    "    \n",
    "    def has_period(self) -> bool:\n",
    "        has_period = False\n",
    "        doc_entity_labels = [ent.label_ for ent in self.__doc.ents]\n",
    "        \n",
    "        if \"DATE\" in self.__doc_entities or \"TIME\" in self.__doc_entities:\n",
    "            has_period = True\n",
    "        \n",
    "        return has_period\n",
    "    \n",
    "    \n",
    "    def __set_doc_tense(self):\n",
    "        for token in self.__doc:\n",
    "            if token.tag_ == \"VBD\" or token.tag_ == \"VBN\" or token.text in Constants.PAST_TENSE:\n",
    "                self.__tense = Constants.TENSE.get(\"past\")\n",
    "                \n",
    "                break\n",
    "        \n",
    "        if \"DATE\" not in self.__doc_entities and \"TIME\" in self.__doc_entities:\n",
    "            self.__tense = Constants.TENSE.get(\"future\")\n",
    "        \n",
    "        return None    \n",
    "    \n",
    "    \n",
    "    def __get_doc_periods(self) -> list:\n",
    "        available_periods = []\n",
    "        available_periods = [(ent.as_doc(), ent.label_) for ent in self.__doc.ents if ent.label_ == \"DATE\"]\n",
    "        available_periods = [(SpacyDoc.getInstance(doc=\"today\"), \"TIME\")] if not available_periods else available_periods\n",
    "        \n",
    "        return available_periods\n",
    "    \n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "            VBD -> past test\n",
    "            VBN -> past participle\n",
    "        \"\"\"\n",
    "        if not self.has_period():\n",
    "            return None, None, None, None\n",
    "        \n",
    "        self.__set_doc_tense()\n",
    "        periods = self.__get_doc_periods()\n",
    "        \n",
    "        period = periods[0][0]\n",
    "        start_date, stop_date = self.__get_period_date_range(doc=period)\n",
    "        \n",
    "        return start_date, stop_date, period, self.__tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2021, 2, 9), datetime.date(2021, 2, 9), today, 'future_tense']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, stop, period, tense = PeriodExtractor(corpus=\"what was my revenue at 4pm\").extract()\n",
    "[start, stop, period, tense]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Chunking & Chinking</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkParser:\n",
    "    def __init__(self, doc: str, subject: str = None):\n",
    "        self.doc = doc\n",
    "        self.subject = subject\n",
    "        \n",
    "        self.found_chunk_labels = []\n",
    "    \n",
    "    def __get_grammar_rules(self):\n",
    "        DEFAULT_MATCHER = r\"\"\"  {np_tag}: {np_include}  # This is a noun phrase chunk\n",
    "                                \n",
    "                                {np_exclude}  # This is chinking regex\n",
    "                                \n",
    "                                {period_tag}: {period}\n",
    "                                {wh_pro_tag}: {wh_pro} # [\"who\", \"which\", \"what\"]\n",
    "                                {wh_det_tag}: {wh_det} # [\"what\", \"which\", \"whose\"]\n",
    "                                {wh_adv_tag}: {wh_adv} # [\"how\", \"why\", \"where\", \"when\"]\n",
    "                            \"\"\".format(np_include=Constants.NOUN_PHRASE_REGX_PATTERN.get(\"include\"), np_tag=Constants.NOUN_PHRASE_REGX_PATTERN.get(\"tag\"), np_exclude=Constants.NOUN_PHRASE_REGX_PATTERN.get(\"exclude\"), period=Constants.PERIOD_REGX_PATTERN.get(\"include\"), period_tag=Constants.PERIOD_REGX_PATTERN.get(\"tag\"), wh_pro=Constants.WH_PRO_REGX_PATTERN.get(\"include\"), wh_pro_tag=Constants.WH_PRO_REGX_PATTERN.get(\"tag\"), wh_det=Constants.WH_DET_REGX_PATTERN.get(\"include\"), wh_det_tag=Constants.WH_DET_REGX_PATTERN.get(\"tag\"), wh_adv=Constants.WH_ADV_REGX_PATTERN.get(\"include\"), wh_adv_tag=Constants.WH_ADV_REGX_PATTERN.get(\"tag\"))\n",
    "        \n",
    "        search = r\"\"\"\n",
    "                      {search_tag}: {search_1}\n",
    "                      {search_tag}: {search_2}\n",
    "\n",
    "                      {exclude}\n",
    "                \"\"\".format(search_tag=Constants.SEARCH_REGX_PATTERN.get(\"tag\"), search_1=Constants.SEARCH_REGX_PATTERN.get(\"include\")[0], search_2=Constants.SEARCH_REGX_PATTERN.get(\"include\")[1], exclude=Constants.SEARCH_REGX_PATTERN.get(\"exclude\"))\n",
    "        \n",
    "        kpi = r\"\"\" \n",
    "                    {kpi_exact_tag}: {kpi_exact_1}\n",
    "                    {kpi_exact_tag}: {kpi_exact_2}\n",
    "                \"\"\".format(kpi_exact_tag=Constants.KPI_EXACT_REGX_PATTERN.get(\"tag\"), kpi_exact_1=Constants.KPI_EXACT_REGX_PATTERN.get(\"include\")[0], kpi_exact_2=Constants.KPI_EXACT_REGX_PATTERN.get(\"include\")[1])\n",
    "        \n",
    "        return [search, kpi, DEFAULT_MATCHER]\n",
    "    \n",
    "    \n",
    "    def __chunk_extractor(self) -> tuple:\n",
    "        grammar_rules = self.__get_grammar_rules()\n",
    "        chunks = []\n",
    "        \n",
    "        for grammar_rule in grammar_rules:\n",
    "            chunks.append(self.__parser(grammar=grammar_rule))\n",
    "        \n",
    "        chunks = list(set([chunk for chunk_ in chunks for chunk in chunk_]))\n",
    "        labels = [chunk[0] for chunk in chunks]\n",
    "        \n",
    "        return chunks, labels\n",
    "    \n",
    "    \n",
    "    def __nltk_grammer_selector(self, subject: str = None) -> str:\n",
    "        DEFAULT_MATCHER = r\"\"\"  {np_tag}: {np_include}  # This is a noun phrase chunk\n",
    "                                \n",
    "                                {np_exclude}  # This is chinking regex\n",
    "                                \n",
    "                                {period_tag}: {period}\n",
    "                                {wh_pro_tag}: {wh_pro} # [\"who\", \"which\", \"what\"]\n",
    "                                {wh_det_tag}: {wh_det} # [\"what\", \"which\", \"whose\"]\n",
    "                                {wh_adv_tag}: {wh_adv} # [\"how\", \"why\", \"where\", \"when\"]\n",
    "                            \"\"\".format(np_include=Constants.NOUN_PHRASE_REGX_PATTERN.get(\"include\"), np_tag=Constants.NOUN_PHRASE_REGX_PATTERN.get(\"tag\"), np_exclude=Constants.NOUN_PHRASE_REGX_PATTERN.get(\"exclude\"), period=Constants.PERIOD_REGX_PATTERN.get(\"include\"), period_tag=Constants.PERIOD_REGX_PATTERN.get(\"tag\"), wh_pro=Constants.WH_PRO_REGX_PATTERN.get(\"include\"), wh_pro_tag=Constants.WH_PRO_REGX_PATTERN.get(\"tag\"), wh_det=Constants.WH_DET_REGX_PATTERN.get(\"include\"), wh_det_tag=Constants.WH_DET_REGX_PATTERN.get(\"tag\"), wh_adv=Constants.WH_ADV_REGX_PATTERN.get(\"include\"), wh_adv_tag=Constants.WH_ADV_REGX_PATTERN.get(\"tag\"))\n",
    "        \n",
    "        matchers = {\n",
    "            \"search\": r\"\"\"\n",
    "                          {search_tag}: {search_1}\n",
    "                          {search_tag}: {search_2}\n",
    "                          \n",
    "                          {exclude}\n",
    "                        \"\"\".format(search_tag=Constants.SEARCH_REGX_PATTERN.get(\"tag\"), search_1=Constants.SEARCH_REGX_PATTERN.get(\"include\")[0], search_2=Constants.SEARCH_REGX_PATTERN.get(\"include\")[1], exclude=Constants.SEARCH_REGX_PATTERN.get(\"exclude\")),\n",
    "            \"kpi\": r\"\"\" \n",
    "                        {kpi_exact_tag}: {kpi_exact_1}\n",
    "                        {kpi_exact_tag}: {kpi_exact_2}\n",
    "                    \"\"\".format(kpi_exact_tag=Constants.KPI_EXACT_REGX_PATTERN.get(\"tag\"), kpi_exact_1=Constants.KPI_EXACT_REGX_PATTERN.get(\"include\")[0], kpi_exact_2=Constants.KPI_EXACT_REGX_PATTERN.get(\"include\")[1]),\n",
    "        }\n",
    "        \n",
    "        if subject is not None:\n",
    "            return matchers.get(subject, DEFAULT_MATCHER)\n",
    "        \n",
    "        return matchers.get(self.subject, DEFAULT_MATCHER)\n",
    "    \n",
    "    @staticmethod\n",
    "    def noun_extractor(doc: str) -> str:\n",
    "        subj, pred, obj = SPO(doc=doc).main()\n",
    "        entities = [subj, obj]\n",
    "        entities = filter(lambda x: x if x is not None else 0, entities)\n",
    "        entities = [entity for entity in entities]\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def __parser(self, grammar: str):\n",
    "        parser = nltk.RegexpParser(grammar=grammar)\n",
    "        chunked = parser.parse(nltk.pos_tag(nltk.tokenize.word_tokenize(self.doc)))\n",
    "        chunk_labels = []\n",
    "        \n",
    "        for chunk in chunked.subtrees():\n",
    "            chunk_phrase = \" \".join(word for word in [text[0] for text in chunk.leaves()])\n",
    "            noun_subject = None\n",
    "            \n",
    "            try:\n",
    "                noun_subject = self.noun_extractor(chunk_phrase)[0]\n",
    "                noun_subject = noun_subject.strip()\n",
    "                \n",
    "            except IndexError:\n",
    "                pass\n",
    "                \n",
    "            finally:\n",
    "                \n",
    "                chunk_labels.append((chunk.label(), chunk_phrase, noun_subject))\n",
    "        \n",
    "        return chunk_labels\n",
    "    \n",
    "    def main(self):\n",
    "#         pos = nltk.pos_tag(nltk.tokenize.word_tokenize(self.doc))\n",
    "#         print(pos)\n",
    "#         displacy.render(SpacyDoc.getInstance(doc=self.doc), style=\"ent\", jupyter=True)\n",
    "#         grammar = self.__nltk_grammer_selector()\n",
    "#         chunk_labels = self.__parser(grammar=grammar)\n",
    "#         chunk_labels = chunk_labels if len(chunk_labels) > 1 else self.__parser(grammar=self.__nltk_grammer_selector(subject=\"default\"))\n",
    "        \n",
    "        return self.__chunk_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('revenue', 'revenue', None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPO(doc=\"revenue\").main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('NP', 'income', 'income'),\n",
       "  ('DEFN', 'income', 'income'),\n",
       "  ('DEFN', 'help', None),\n",
       "  ('NP', 'help', None),\n",
       "  ('S', 'Need some help on income .', 'income')],\n",
       " ['NP', 'DEFN', 'DEFN', 'NP', 'S'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ChunkParser(doc=\"Need some help on income.\", subject=\"search\").main()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Google search</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query):\n",
    "    user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    options.add_argument(f'user-agent={user_agent}')\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--allow-running-insecure-content')\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--proxy-server='direct://'\")\n",
    "    options.add_argument(\"--proxy-bypass-list=*\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('headless')\n",
    "    \n",
    "    driver = webdriver.Chrome(\"/usr/local/share/chromedriver\",options=options)\n",
    "    driver.get(\"https://www.google.com/search?q=\"+query.lower())\n",
    "    \n",
    "    results = None\n",
    "    responses = [\"From a google search on\", \"According to a google research on\", \"Here is what I found on\"]\n",
    "    response_head = random.choice(responses)\n",
    "    \n",
    "    try:\n",
    "        result_obj = driver.find_element_by_xpath(\"//div[@data-attrid='description']\").find_elements_by_tag_name(\"span\")\n",
    "        \n",
    "        for obj in result_obj:\n",
    "            if len(obj.text.split()) < 5:\n",
    "                continue\n",
    "            \n",
    "            results = obj.text\n",
    "        \n",
    "        try:\n",
    "            if results:\n",
    "                results = TextBlob(u\"{text}\".format(text=results)).translate(to=\"en\")\n",
    "        \n",
    "        except NotTranslated:\n",
    "            pass\n",
    "        \n",
    "        finally:\n",
    "            results = f'{response_head} \"{query}\"; {results}'\n",
    "            return results\n",
    "        \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is what I found on \"expenses\"; Expenses are the outflow of money, or any form of wealth in general, to another person or group to pay for an item or service, or for a category of costs. For a tenant, rent is an expense. For students or parents, tuition is an expense.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search(\"expenses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Bot Responses</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Responses:\n",
    "    def __init__(self, subject: str, chunk_labels: list, chunk_tags: list, period: str, tense: str,  noun_subject: str = None, start_date: str = None, stop_date: str = None):\n",
    "        self.subject = subject\n",
    "        self.chunk_labels = chunk_labels\n",
    "        self.chunk_tags = chunk_tags\n",
    "        self.period = period\n",
    "        self.tense = tense\n",
    "        self.noun_subject = noun_subject\n",
    "        \n",
    "        self.start_date = start_date\n",
    "        self.stop_date = stop_date\n",
    "        self.suggested_periods = [\"today\", \"this week\", \"this month\"]\n",
    "    \n",
    "    def __db_get_kpi_value(self):\n",
    "        \"\"\"\n",
    "            Still have to build this function using \"self.start_date & self.stop_date\"\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"$1000\"\n",
    "    \n",
    "    def kpi(self, tag: str, kpi: str):\n",
    "        tags = {\n",
    "            \"what_exactly_period\": \"What exactly do you want to know about your {kpi} for {period}?\".format(period=self.period, kpi=kpi),\n",
    "            \"what_exactly_no_period\": \"What exactly do you want to know about your {kpi}?\".format(kpi=kpi),\n",
    "            \"ask_period\": \"Do you mean your {kpi} for {period}? Or when exactly?\".format(kpi=kpi, period=random.choice(self.suggested_periods) if not self.period else self.period),\n",
    "            \"success\": \"Your {kpi} for {period} {tense} {kpi_value}.\".format(period=self.period, kpi=kpi, tense=\"was\" if self.tense == Constants.TENSE.get(\"past\") else \"is\", kpi_value=self.__db_get_kpi_value()),\n",
    "        }\n",
    "        \n",
    "        return tags.get(tag)\n",
    "    \n",
    "    def search(self, query):\n",
    "        response = google_search(query=query)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def error(self, tag: str, kpi: str = None):\n",
    "        tags = {\n",
    "            \"wh_pronoun_determiner_error\": \"Do you mean your {kpi} for {period}?\".format(kpi=kpi, period=random.choice(self.suggested_periods) if not self.period else self.period),\n",
    "        }\n",
    "        \n",
    "        return tags.get(tag)\n",
    "    \n",
    "    def response(self):\n",
    "        response = None\n",
    "        success = False\n",
    "        done = True\n",
    "        \n",
    "        wh_valid_words = Constants.WH_PRO_REGX_PATTERN.get(\"valid_words\").copy()\n",
    "        wh_valid_words.extend(Constants.WH_DET_REGX_PATTERN.get(\"valid_words\"))\n",
    "        wh_valid_words = list(set(wh_valid_words))\n",
    "        wh_tags = [\"WDT\", \"WP\", \"WPR\", \"WP$\", \"WRB\"]\n",
    "                \n",
    "        for chunk in self.chunk_labels:\n",
    "            [label, phrase, noun_subject] = chunk\n",
    "            phrase_pos = [tag for word, tag in nltk.pos_tag(nltk.tokenize.word_tokenize(phrase))]\n",
    "            \n",
    "            if label == Constants.SEARCH_REGX_PATTERN.get(\"tag\"):\n",
    "                response = self.search(query=noun_subject)\n",
    "                success = True if response is not None else False\n",
    "                done = True\n",
    "        \n",
    "        return response, success, done\n",
    "    \n",
    "#     def response(self):\n",
    "#         response = None\n",
    "#         success = False\n",
    "#         done = True\n",
    "        \n",
    "#         wh_valid_words = Constants.WH_PRO_REGX_PATTERN.get(\"valid_words\").copy()\n",
    "#         wh_valid_words.extend(Constants.WH_DET_REGX_PATTERN.get(\"valid_words\"))\n",
    "#         wh_valid_words = list(set(wh_valid_words))\n",
    "#         wh_tags = [\"WDT\", \"WP\", \"WPR\", \"WP$\", \"WRB\"]\n",
    "        \n",
    "#         for chunk in self.chunk_labels:\n",
    "#             [label, phrase, noun_subject] = chunk\n",
    "#             phrase_pos = [tag for word, tag in nltk.pos_tag(nltk.tokenize.word_tokenize(phrase))]\n",
    "            \n",
    "#             if label in [Constants.WH_PRO_REGX_PATTERN.get(\"tag\"), Constants.WH_DET_REGX_PATTERN.get(\"tag\")] and self.noun_subject in Constants.KPI_TYPES:\n",
    "#                 response = self.kpi(tag=\"success\", kpi=self.noun_subject)\n",
    "#                 success = True\n",
    "#                 done = True\n",
    "#                 break\n",
    "\n",
    "#             if label is Constants.KPI_EXACT_REGX_PATTERN.get(\"tag\"):\n",
    "#                 valid_wh_word = False\n",
    "#                 success = True\n",
    "                \n",
    "#                 if self.period is not None:\n",
    "#                     for wh_word in wh_valid_words:\n",
    "#                         if wh_word in phrase:\n",
    "#                             valid_wh_word = True\n",
    "#                             break\n",
    "                    \n",
    "#                     if valid_wh_word:\n",
    "#                         response = self.kpi(tag=\"success\", kpi=noun_subject)\n",
    "#                         done = True\n",
    "#                         break\n",
    "                    \n",
    "#                     else:\n",
    "#                         invalid_wh_tag = False\n",
    "                        \n",
    "#                         for pos_tag in phrase_pos:\n",
    "#                             if pos_tag in wh_tags:\n",
    "#                                 invalid_wh_tag = True\n",
    "#                                 break\n",
    "                        \n",
    "#                         response = None if invalid_wh_tag else self.kpi(tag=\"success\", kpi=noun_subject)\n",
    "#                         success = False if invalid_wh_tag else True\n",
    "#                         done = True\n",
    "#                         break\n",
    "                \n",
    "#                 else:\n",
    "# #                     print(self.period, \"############\")\n",
    "#                     response = self.kpi(tag=\"ask_period\", kpi=noun_subject)\n",
    "#                     done = False\n",
    "#                     break\n",
    "            \n",
    "#             elif label is not Constants.KPI_EXACT_REGX_PATTERN.get(\"tag\"):\n",
    "#                 if self.subject == \"kpi\":\n",
    "#                     response  = self.kpi(tag=\"what_exactly_period\", kpi=self.noun_subject) if self.period is not None else self.kpi(tag=\"what_exactly_no_period\", kpi=self.noun_subject)\n",
    "#                     success = True\n",
    "#                     done = False\n",
    "\n",
    "#                 else:\n",
    "#                     response = None\n",
    "#                     success = False\n",
    "#                     done = True\n",
    "            \n",
    "#             elif label is Constants.SEARCH_REGX_PATTERN.get(\"tag\"):\n",
    "#                 response = self.search(query=noun_subject)\n",
    "#                 success = True if response is not None else False\n",
    "#                 done = True\n",
    "            \n",
    "#             else:\n",
    "#                 pass\n",
    "                \n",
    "                \n",
    "#         return response, success, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Get Subject Context</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextExtractor:\n",
    "    def __init__(self, data_path, model_path):\n",
    "        try:\n",
    "            with open(data_path, 'rb') as file:\n",
    "                self.words, self.classes, self.documents = pickle.load(file)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise FileNotFoundError(f\"{data_path} doesn't exist\")\n",
    "            \n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise FileNotFoundError(f\"{model_path} doesn't exist\")\n",
    "           \n",
    "    @property\n",
    "    def ignore_words(self):\n",
    "        return [\"'s\"] + [s for s in string.punctuation]\n",
    "    \n",
    "    def clean_up_sentence(self, sentence):\n",
    "        stemmer_ = nltk.stem.lancaster.LancasterStemmer()\n",
    "        \n",
    "        s_words = nltk.word_tokenize(sentence)\n",
    "        s_words = [stemmer_.stem(w.lower()) for w in s_words if w not in self.ignore_words]\n",
    "\n",
    "        return s_words\n",
    "\n",
    "    def bow(self, sentence, words, show_details=True):\n",
    "        s_words = self.clean_up_sentence(sentence)\n",
    "\n",
    "        bag = [0 for _ in range(len(words))]\n",
    "\n",
    "        for s in s_words:\n",
    "            for i, w in enumerate(words):\n",
    "                if w == s:\n",
    "                    bag[i] = 1\n",
    "\n",
    "                    if show_details:\n",
    "                        print(f\"found in bag: {s}\")\n",
    "\n",
    "        return np.array(bag)\n",
    "    \n",
    "    def classify_local(self, sentence):\n",
    "        ERROR_THRESHOLD = 0.6\n",
    "\n",
    "        input_data = pd.DataFrame([self.bow(sentence, self.words, show_details=False)], dtype=float, index=['input'])\n",
    "        input_data = input_data.values.reshape(-1, 1, input_data.shape[1])\n",
    "\n",
    "        results = self.model.predict([input_data])[0]\n",
    "\n",
    "        #filter out prediction below a threshold, and provide intent index\n",
    "        results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "\n",
    "        # sort by strength of probability\n",
    "        results.sort(key=lambda x: x[1], reverse=False)\n",
    "\n",
    "        return_list = []\n",
    "        for r in results:\n",
    "            return_list.append((self.classes[r[0]], str(r[1])))\n",
    "\n",
    "        return return_list\n",
    "\n",
    "    def run(self, doc: str):\n",
    "        try:\n",
    "            results = self.classify_local(doc)[0]\n",
    "            return results[0], doc\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise(e)\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f77644b8160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('revenue', 'Insights about my earnings for this week, please.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_instance = ContextExtractor(data_path=Constants.CONTEXT_MODEL_DATA_PATH, model_path=Constants.CONTEXT_MODEL_PATH)\n",
    "context_instance.run(doc=\"Insights about my earnings for this week, please.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">Test Bot</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter query:  tell me something about expenses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7740453040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Context: expenses\n",
      "[('S', 'tell me something about expenses', 'expenses'), ('NP', 'something', 'something'), ('NP', 'expenses', 'expenses'), ('DEFN', 'expenses', 'expenses'), ('DEFN', 'something', 'something')]\n",
      "Bot: sorry don't understand you. Please try saying this in another way. \n",
      "\n",
      "success: False \t done: True \t Runtime: 38.54 secs \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter query:  exit\n"
     ]
    }
   ],
   "source": [
    "class Bot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def run(text_lemmatization: bool = False, stopword_removal: bool = False):\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                input_data = input(\"Enter query: \")\n",
    "                                \n",
    "                if input_data.lower() in [\"end\", \"stop\", \"exit\", \"quit\", \"terminate\"] or len(input_data) == 0:\n",
    "                    break\n",
    "                \n",
    "                # start time tracker for performance check\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Normalization of input corpus\n",
    "                normalized_corpus = CorpusNormalization(corpus=input_data).normalize(stopword_removal=stopword_removal, text_lemmatization=text_lemmatization)\n",
    "                \n",
    "                # Doc classifier to know if doc is [sent & type or phrase & type]\n",
    "                doc_type = doc_classifier(doc=normalized_corpus)\n",
    "                \n",
    "                # Context extractor -- This is to know what exactly we are dealing with.\n",
    "                subject, _ = ContextExtractor(data_path=Constants.CONTEXT_MODEL_DATA_PATH, model_path=Constants.CONTEXT_MODEL_PATH).run(doc=normalized_corpus)\n",
    "                print(f\"Context: {subject}\")\n",
    "                \n",
    "                # Pass our normalized doc through our grammar rules -- Returns (chunk_tag, chunk_phrase, noun_subject)\n",
    "                chunk_labels, chunk_tags = ChunkParser(doc=normalized_corpus, subject=subject).main()\n",
    "                print(chunk_labels)\n",
    "                \n",
    "                if subject is None:\n",
    "                    raise GlobalException(None)\n",
    "                \n",
    "                # Period Extractor class\n",
    "                start_date, stop_date, period, tense = PeriodExtractor(corpus=str(normalized_corpus)).extract()\n",
    "                \n",
    "                # Bot response\n",
    "                response, success, done = Responses(subject=subject, chunk_labels=chunk_labels, chunk_tags=chunk_tags, period=period, noun_subject=subject, tense=tense).response()\n",
    "                response = random.choice(Constants.DEFAULT_RESPONSE) if not success and done else response\n",
    "                \n",
    "                print(f\"Bot: {response} \\n\")\n",
    "                \n",
    "                # start time tracker for performance check\n",
    "                stop_time = time.time()\n",
    "                \n",
    "                print(f\"success: {success} \\t done: {done} \\t Runtime: {round(stop_time - start_time, 2)} secs \\n\")\n",
    "            \n",
    "            except (GlobalException, LangDetectException):\n",
    "                print(f\"Bot: {random.choice(Constants.DEFAULT_RESPONSE)} \\n\")\n",
    "                continue\n",
    "\n",
    "# Run test bot\n",
    "Bot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look-up on \"Need some help on income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (DEFN John/NNP of/IN revenue/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"John of revenue\"\n",
    "\n",
    "rule = r\"\"\" DEFN: {<WP.*|WDT|WRB><VB.*><JJ.*>*<NN.*><IN>?<NN.*>?}\n",
    "            DEFN: {<IN|UH>*<VB.*><RP>*<IN|UH>?<DT>?<PRP>?<JJ.*>*<IN>?<NN.*><IN>?<NN.*>?}\n",
    "            DEFN: {<NN.*><IN>*<PRP>?<JJ.*>*<NN.*><NN.*>?}\n",
    "        \"\"\"\n",
    "parser  = nltk.RegexpParser(rule)\n",
    "\n",
    "chunk = parser.parse(nltk.pos_tag(nltk.tokenize.word_tokenize(text)))\n",
    "# for tree in chunk.subtrees():\n",
    "#     print(tree.label())\n",
    "# chunk.label()\n",
    "\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Definations', 'NNS'), ('of', 'IN'), ('revenue', 'NN')] \n",
      "\n",
      "['Definations', 'revenue']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>tag</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Definations</td>\n",
       "      <td>defination</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>revenue</td>\n",
       "      <td>revenue</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text       lemma  tag   pos   dep\n",
       "0  Definations  defination  NNS  NOUN  ROOT\n",
       "1           of          of   IN   ADP  prep\n",
       "2      revenue     revenue   NN  NOUN  pobj"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Definations of revenue\"\n",
    "doc = SpacyDoc.getInstance(doc=text)\n",
    "\n",
    "print(nltk.pos_tag(nltk.tokenize.word_tokenize(text)), \"\\n\")\n",
    "\n",
    "data = [(token.text, token.lemma_, token.tag_, token.pos_, token.dep_) for token in doc if not token.is_punct]\n",
    "noun_phrases = [noun.text for noun in doc.noun_chunks]\n",
    "df = pd.DataFrame(data, columns=[\"text\", \"lemma\", \"tag\", \"pos\", \"dep\"])\n",
    "\n",
    "print(noun_phrases)\n",
    "\n",
    "# displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "# displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
